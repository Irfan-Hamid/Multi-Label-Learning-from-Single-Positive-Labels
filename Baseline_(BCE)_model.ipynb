{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import hamming_loss, f1_score, jaccard_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sinusoidal_encoding(coordinates):\n",
    "    if coordinates.shape[1] != 2:\n",
    "        raise ValueError(\"Input array must have two columns (lon, lat)\")\n",
    "\n",
    "    lon = 2 * (coordinates[:, 0] - coordinates[:, 0].min()) / (coordinates[:, 0].max() - coordinates[:, 0].min()) - 1\n",
    "    lat = 2 * (coordinates[:, 1] - coordinates[:, 1].min()) / (coordinates[:, 1].max() - coordinates[:, 1].min()) - 1\n",
    "\n",
    "\n",
    "    encoded_lon = np.column_stack([np.sin(np.pi * lon), np.cos(np.pi * lon)])\n",
    "    encoded_lat = np.column_stack([np.sin(np.pi * lat), np.cos(np.pi * lat)])\n",
    "\n",
    "    encoded_vector = np.concatenate([encoded_lon, encoded_lat], axis=1)\n",
    "\n",
    "    return encoded_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data    \n",
    "data = np.load('data/species_train.npz')\n",
    "train_locs = data['train_locs']  # 2D array, rows are number of datapoints and \n",
    "                                 # columns are \"latitude\" and \"longitude\"\n",
    "train_ids = data['train_ids']    # 1D array, entries are the ID of the species \n",
    "                                 # that is present at the corresponding location in train_locs\n",
    "species = data['taxon_ids']      # list of species IDe. Note these do not necessarily start at 0 (or 1)\n",
    "species_names = dict(zip(data['taxon_ids'], data['taxon_names']))  # latin names of species \n",
    "\n",
    "# loading test data \n",
    "data_test = np.load('data/species_test.npz', allow_pickle=True)\n",
    "test_locs = data_test['test_locs']    # 2D array, rows are number of datapoints \n",
    "                                      # and columns are \"latitude\" and \"longitude\"\n",
    "# data_test['test_pos_inds'] is a list of lists, where each list corresponds to \n",
    "# the indices in test_locs where a given species is present, it can be assumed \n",
    "# that they are not present in the other locations \n",
    "test_pos_inds = dict(zip(data_test['taxon_ids'], data_test['test_pos_inds']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneHotEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irfan_lkd13dt\\amlcoursework\\Baseline_(BCE)_model.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m encoder \u001b[39m=\u001b[39m OneHotEncoder(sparse_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Fittinh and transforming the train_ids to one-hot encoded labels\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_ids_reshaped \u001b[39m=\u001b[39m train_ids\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OneHotEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fittinh and transforming the train_ids to one-hot encoded labels\n",
    "train_ids_reshaped = train_ids.reshape(-1, 1)\n",
    "one_hot_encoded_ids = encoder.fit_transform(train_ids_reshaped)\n",
    "X = sinusoidal_encoding(train_locs)\n",
    "y = one_hot_encoded_ids\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size :  torch.Size([217629, 4])\n",
      "y_train size :  torch.Size([217629, 500])\n",
      "X_val size :  torch.Size([54408, 4])\n",
      "y_val size :  torch.Size([54408, 500])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_val = torch.FloatTensor(y_val)\n",
    "print(\"X_train size : \", X_train.data.size())\n",
    "print(\"y_train size : \", y_train.data.size())\n",
    "print(\"X_val size : \", X_val.data.size())\n",
    "print(\"y_val size : \", y_val.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with standard BCE LOSS for Multi Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.6933140158653259, Val Loss: 0.6440598368644714\n",
      "Epoch: 1, Train Loss: 0.6440410017967224, Val Loss: 0.5433555245399475\n"
     ]
    }
   ],
   "source": [
    "class MultiLabelNeuralNet_BCE(nn.Module):\n",
    "    def __init__(self, in_features=4, num_classes=500):\n",
    "        super().__init__()\n",
    "        self.layer_one = nn.Linear(in_features, 32)\n",
    "        self.layer_two = nn.Linear(32,64)\n",
    "        self.layer_three = nn.Linear(64,128)\n",
    "        self.layer_four = nn.Linear(128,200)\n",
    "        self.output_layer = nn.Linear(200, num_classes)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.layer_one.weight)\n",
    "        nn.init.xavier_uniform_(self.layer_two.weight)\n",
    "        nn.init.xavier_uniform_(self.layer_three.weight)\n",
    "        nn.init.xavier_uniform_(self.layer_four.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.layer_one(x))\n",
    "        x = nn.functional.relu(self.layer_two(x))\n",
    "        x = nn.functional.relu(self.layer_three(x))\n",
    "        x = nn.functional.relu(self.layer_four(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = MultiLabelNeuralNet_BCE()  \n",
    "\n",
    "# Define the BCEWithLogitsLoss loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.007)\n",
    "\n",
    "num_epochs = 2\n",
    "train_losses = []\n",
    "validation_losses =[]\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    # Forward pass\n",
    "    outputs = model.forward(X_train)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs,y_train)\n",
    "\n",
    "    train_losses.append(loss.detach().numpy())\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model.forward(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "        validation_losses.append(val_loss.item())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch}, Train Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encoded_matrix(test_locs, test_pos_inds):\n",
    "\n",
    "    unique_species_ids = sorted(test_pos_inds.keys())\n",
    "\n",
    "    one_hot_encoded_matrix = np.zeros((len(test_locs), len(unique_species_ids)))\n",
    "\n",
    "    species_id_to_index = {species_id: index for index, species_id in enumerate(unique_species_ids)}\n",
    "\n",
    "    for species_id, indices in test_pos_inds.items():\n",
    "        one_hot_encoded_matrix[indices, species_id_to_index[species_id]] = 1\n",
    "\n",
    "    return one_hot_encoded_matrix\n",
    "\n",
    "one_hot_test_data = create_one_hot_encoded_matrix(test_locs, test_pos_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test size :  torch.Size([288122, 4])\n",
      "y_test size :  torch.Size([288122, 500])\n"
     ]
    }
   ],
   "source": [
    "X_t = sinusoidal_encoding(test_locs)\n",
    "y_t = one_hot_test_data\n",
    "X.shape, y.shape\n",
    "X_tt = torch.FloatTensor(X_t)\n",
    "y_tt = torch.FloatTensor(y_t)\n",
    "print(\"X_test size : \", X_tt.data.size())\n",
    "print(\"y_test size : \", y_tt.data.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irfan_lkd13dt\\amlcoursework\\Baseline_(BCE)_model.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     y_test \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(X_tt)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     test_loss \u001b[39m=\u001b[39m criterion(y_test,y_tt)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_test = model.forward(X_tt)\n",
    "    test_loss = criterion(y_test,y_tt)\n",
    "\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irfan_lkd13dt\\amlcoursework\\Baseline_(BCE)_model.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m predictions \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39msigmoid(y_test) \u001b[39m>\u001b[39m threshold)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m predictions_np \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_true_np \u001b[39m=\u001b[39m y_tt\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "predictions = (torch.sigmoid(y_test) > threshold).float()\n",
    "\n",
    "predictions_np = predictions.numpy()\n",
    "y_true_np = y_tt.numpy()\n",
    "\n",
    "predictions = predictions.numpy()  \n",
    "true_labels= y_tt.numpy() \n",
    "\n",
    "y_true = true_labels\n",
    "y_pred = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.0195\n",
      "Hamming Accuracy: 0.9805\n",
      "Jaccard Score_micro: 0.0035\n",
      "Jaccard Score-macro: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irfan_lkd13dt\\miniconda3\\envs\\aml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       619\n",
      "           1       0.00      0.00      0.00      2234\n",
      "           2       0.00      0.00      0.00      2926\n",
      "           3       0.00      0.00      0.00       685\n",
      "           4       0.00      0.00      0.00       850\n",
      "           5       0.00      0.00      0.00       631\n",
      "           6       0.00      0.00      0.00      4960\n",
      "           7       0.00      0.00      0.00      6939\n",
      "           8       0.00      0.00      0.00       253\n",
      "           9       0.00      0.00      0.00      3012\n",
      "          10       0.00      0.00      0.00       814\n",
      "          11       0.00      0.00      0.00      4060\n",
      "          12       0.00      0.00      0.00      1759\n",
      "          13       0.00      0.00      0.00      1605\n",
      "          14       0.00      0.00      0.00       277\n",
      "          15       0.00      0.00      0.00      4819\n",
      "          16       0.00      0.00      0.00      3269\n",
      "          17       0.00      0.00      0.00      1444\n",
      "          18       0.00      0.00      0.00      1582\n",
      "          19       0.00      0.00      0.00       186\n",
      "          20       0.00      0.00      0.00       608\n",
      "          21       0.00      0.00      0.00      3618\n",
      "          22       0.00      0.00      0.00      1476\n",
      "          23       0.00      0.00      0.00       538\n",
      "          24       0.00      0.00      0.00      8367\n",
      "          25       0.00      0.00      0.00       292\n",
      "          26       0.00      0.00      0.00      1646\n",
      "          27       0.00      0.00      0.00    152474\n",
      "          28       0.00      0.00      0.00     43332\n",
      "          29       0.00      0.00      0.00     21911\n",
      "          30       0.00      0.00      0.00       101\n",
      "          31       0.00      0.00      0.00       176\n",
      "          32       0.00      0.00      0.00     97305\n",
      "          33       0.00      0.00      0.00      7561\n",
      "          34       0.00      0.00      0.00     62302\n",
      "          35       0.00      0.00      0.00      1929\n",
      "          36       0.24      0.49      0.32     16511\n",
      "          37       0.00      0.00      0.00      4560\n",
      "          38       0.00      0.00      0.00     12274\n",
      "          39       0.00      0.00      0.00      9836\n",
      "          40       0.00      0.00      0.00      5139\n",
      "          41       0.00      0.00      0.00      4468\n",
      "          42       0.00      0.00      0.00      2375\n",
      "          43       0.00      0.00      0.00       842\n",
      "          44       0.00      0.00      0.00      8011\n",
      "          45       0.00      0.00      0.00      6073\n",
      "          46       0.00      0.00      0.00      1288\n",
      "          47       0.00      0.00      0.00      1982\n",
      "          48       0.00      0.00      0.00      3813\n",
      "          49       0.00      0.00      0.00      4804\n",
      "          50       0.00      0.00      0.00      8535\n",
      "          51       0.00      0.00      0.00      7511\n",
      "          52       0.00      0.00      0.00       921\n",
      "          53       0.00      0.00      0.00      5229\n",
      "          54       0.00      0.00      0.00      2020\n",
      "          55       0.00      0.00      0.00       905\n",
      "          56       0.00      0.00      0.00       151\n",
      "          57       0.00      0.00      0.00        40\n",
      "          58       0.00      0.00      0.00       526\n",
      "          59       0.00      0.00      0.00         8\n",
      "          60       0.00      0.00      0.00      5589\n",
      "          61       0.00      0.00      0.00      5075\n",
      "          62       0.00      0.00      0.00       149\n",
      "          63       0.00      0.00      0.00      1288\n",
      "          64       0.00      0.00      0.00      1617\n",
      "          65       0.00      0.00      0.00      1724\n",
      "          66       0.00      0.00      0.00      7119\n",
      "          67       0.00      0.00      0.00       102\n",
      "          68       0.00      0.00      0.00     16819\n",
      "          69       0.00      0.00      0.00      2114\n",
      "          70       0.00      0.00      0.00       735\n",
      "          71       0.00      0.00      0.00      8453\n",
      "          72       0.00      0.00      0.00       365\n",
      "          73       0.00      0.00      0.00      1216\n",
      "          74       0.00      0.00      0.00       358\n",
      "          75       0.00      0.00      0.00      5838\n",
      "          76       0.00      0.00      0.00      4461\n",
      "          77       0.00      0.00      0.00      3990\n",
      "          78       0.00      0.00      0.00      4572\n",
      "          79       0.00      0.00      0.00      2694\n",
      "          80       0.00      0.00      0.00       307\n",
      "          81       0.00      0.00      0.00       364\n",
      "          82       0.00      0.00      0.00      5967\n",
      "          83       0.00      0.00      0.00      2765\n",
      "          84       0.00      0.00      0.00      4750\n",
      "          85       0.00      0.00      0.00      3618\n",
      "          86       0.00      0.00      0.00      1390\n",
      "          87       0.00      0.00      0.00      3272\n",
      "          88       0.00      0.00      0.00       123\n",
      "          89       0.00      0.00      0.00      3498\n",
      "          90       0.00      0.00      0.00       133\n",
      "          91       0.00      0.00      0.00       809\n",
      "          92       0.00      0.00      0.00      3424\n",
      "          93       0.00      0.00      0.00      2838\n",
      "          94       0.00      0.00      0.00       846\n",
      "          95       0.00      0.00      0.00       551\n",
      "          96       0.00      0.00      0.00        12\n",
      "          97       0.00      0.00      0.00       199\n",
      "          98       0.00      0.00      0.00       321\n",
      "          99       0.00      0.00      0.00      4880\n",
      "         100       0.00      0.00      0.00       109\n",
      "         101       0.00      0.00      0.00       198\n",
      "         102       0.00      0.00      0.00     16712\n",
      "         103       0.00      0.00      0.00     14318\n",
      "         104       0.00      0.00      0.00        16\n",
      "         105       0.00      0.00      0.00       910\n",
      "         106       0.00      0.00      0.00       334\n",
      "         107       0.00      0.00      0.00      6345\n",
      "         108       0.00      0.00      0.00      5105\n",
      "         109       0.00      0.00      0.00      7153\n",
      "         110       0.00      0.00      0.00      6722\n",
      "         111       0.00      0.07      0.00      2019\n",
      "         112       0.00      0.00      0.00      1493\n",
      "         113       0.00      0.00      0.00      2870\n",
      "         114       0.00      0.00      0.00      1739\n",
      "         115       0.00      0.00      0.00       191\n",
      "         116       0.00      0.00      0.00      8747\n",
      "         117       0.00      0.00      0.00      6706\n",
      "         118       0.00      0.00      0.00      7829\n",
      "         119       0.00      0.00      0.00      5233\n",
      "         120       0.00      0.00      0.00     15587\n",
      "         121       0.00      0.00      0.00       340\n",
      "         122       0.00      0.00      0.00        64\n",
      "         123       0.00      0.00      0.00       159\n",
      "         124       0.00      0.00      0.00       378\n",
      "         125       0.00      0.00      0.00       957\n",
      "         126       0.00      0.00      0.00       526\n",
      "         127       0.00      0.00      0.00      3496\n",
      "         128       0.00      0.00      0.00       222\n",
      "         129       0.00      0.00      0.00     12457\n",
      "         130       0.00      0.00      0.00      9197\n",
      "         131       0.00      0.00      0.00      9440\n",
      "         132       0.00      0.00      0.00      4972\n",
      "         133       0.00      0.00      0.00        69\n",
      "         134       0.00      0.00      0.00       327\n",
      "         135       0.00      0.00      0.00      3500\n",
      "         136       0.00      0.00      0.00      2194\n",
      "         137       0.00      0.00      0.00      3214\n",
      "         138       0.00      0.00      0.00      3175\n",
      "         139       0.00      0.00      0.00      5683\n",
      "         140       0.00      0.00      0.00        31\n",
      "         141       0.00      0.00      0.00      5017\n",
      "         142       0.00      0.00      0.00      2110\n",
      "         143       0.00      0.00      0.00      3721\n",
      "         144       0.00      0.00      0.00      1746\n",
      "         145       0.00      0.00      0.00       932\n",
      "         146       0.00      0.00      0.00      2739\n",
      "         147       0.00      0.00      0.00       100\n",
      "         148       0.00      0.00      0.00      1074\n",
      "         149       0.00      0.00      0.00       977\n",
      "         150       0.00      0.00      0.00       193\n",
      "         151       0.00      0.00      0.00      1089\n",
      "         152       0.00      0.00      0.00      2367\n",
      "         153       0.00      0.00      0.00      3857\n",
      "         154       0.00      0.00      0.00      4440\n",
      "         155       0.00      0.00      0.00      1793\n",
      "         156       0.00      0.00      0.00      4624\n",
      "         157       0.00      0.00      0.00      1494\n",
      "         158       0.00      0.00      0.00     23943\n",
      "         159       0.00      0.00      0.00      1939\n",
      "         160       0.00      0.00      0.00      1038\n",
      "         161       0.00      0.00      0.00       465\n",
      "         162       0.00      0.00      0.00       662\n",
      "         163       0.00      0.00      0.00      4058\n",
      "         164       0.00      0.00      0.00      1933\n",
      "         165       0.00      0.00      0.00       401\n",
      "         166       0.00      0.00      0.00      3989\n",
      "         167       0.00      0.00      0.00       167\n",
      "         168       0.00      0.00      0.00       908\n",
      "         169       0.00      0.00      0.00      2391\n",
      "         170       0.00      0.00      0.00       835\n",
      "         171       0.00      0.00      0.00       820\n",
      "         172       0.00      0.00      0.00      1512\n",
      "         173       0.00      0.00      0.00       131\n",
      "         174       0.00      0.00      0.00       457\n",
      "         175       0.00      0.00      0.00      2421\n",
      "         176       0.00      0.00      0.00       411\n",
      "         177       0.00      0.00      0.00       326\n",
      "         178       0.00      0.00      0.00       187\n",
      "         179       0.00      0.00      0.00     27558\n",
      "         180       0.00      0.00      0.00      6493\n",
      "         181       0.00      0.00      0.00      6772\n",
      "         182       0.00      0.00      0.00         8\n",
      "         183       0.00      0.00      0.00       236\n",
      "         184       0.00      0.00      0.00      9900\n",
      "         185       0.00      0.00      0.00      8171\n",
      "         186       0.00      0.00      0.00      5895\n",
      "         187       0.00      0.00      0.00       968\n",
      "         188       0.00      0.00      0.00      6820\n",
      "         189       0.00      0.00      0.00      2204\n",
      "         190       0.00      0.00      0.00       747\n",
      "         191       0.00      0.00      0.00      2054\n",
      "         192       0.00      0.00      0.00      3185\n",
      "         193       0.00      0.00      0.00       112\n",
      "         194       0.00      0.00      0.00       580\n",
      "         195       0.00      0.00      0.00       657\n",
      "         196       0.00      0.00      0.00      3184\n",
      "         197       0.00      0.00      0.00      1263\n",
      "         198       0.00      0.00      0.00       477\n",
      "         199       0.00      0.00      0.00      3235\n",
      "         200       0.00      0.00      0.00      2896\n",
      "         201       0.00      0.00      0.00      9207\n",
      "         202       0.00      0.00      0.00      2997\n",
      "         203       0.00      0.00      0.00      4139\n",
      "         204       0.00      0.00      0.00      6192\n",
      "         205       0.00      0.00      0.00       468\n",
      "         206       0.00      0.00      0.00       510\n",
      "         207       0.00      0.00      0.00       269\n",
      "         208       0.00      0.00      0.00      6595\n",
      "         209       0.00      0.00      0.00       295\n",
      "         210       0.00      0.00      0.00       534\n",
      "         211       0.00      0.00      0.00       141\n",
      "         212       0.00      0.00      0.00      3046\n",
      "         213       0.00      0.00      0.00       484\n",
      "         214       0.00      0.00      0.00       253\n",
      "         215       0.00      0.00      0.00        77\n",
      "         216       0.00      0.00      0.00        25\n",
      "         217       0.00      0.00      0.00      5559\n",
      "         218       0.00      0.00      0.00       820\n",
      "         219       0.00      0.00      0.00      1514\n",
      "         220       0.00      0.00      0.00        35\n",
      "         221       0.00      0.00      0.00        12\n",
      "         222       0.00      0.00      0.00      3046\n",
      "         223       0.00      0.00      0.00      6008\n",
      "         224       0.00      0.00      0.00      1277\n",
      "         225       0.00      0.00      0.00       594\n",
      "         226       0.00      0.00      0.00       152\n",
      "         227       0.00      0.00      0.00       150\n",
      "         228       0.00      0.00      0.00      1006\n",
      "         229       0.00      0.00      0.00       732\n",
      "         230       0.00      0.00      0.00       103\n",
      "         231       0.00      0.00      0.00      1626\n",
      "         232       0.00      0.00      0.00      1077\n",
      "         233       0.00      0.00      0.00      1478\n",
      "         234       0.00      0.00      0.00       890\n",
      "         235       0.00      0.00      0.00       306\n",
      "         236       0.00      0.00      0.00       474\n",
      "         237       0.00      0.00      0.00       152\n",
      "         238       0.00      0.00      0.00        98\n",
      "         239       0.00      0.00      0.00       316\n",
      "         240       0.00      0.00      0.00        46\n",
      "         241       0.00      0.00      0.00       208\n",
      "         242       0.00      0.00      0.00      4327\n",
      "         243       0.00      0.00      0.00        43\n",
      "         244       0.00      0.00      0.00       238\n",
      "         245       0.00      0.00      0.00        63\n",
      "         246       0.00      0.00      0.00       504\n",
      "         247       0.00      0.00      0.00        19\n",
      "         248       0.00      0.00      0.00      1416\n",
      "         249       0.00      0.00      0.00        55\n",
      "         250       0.00      0.00      0.00        44\n",
      "         251       0.00      0.00      0.00        32\n",
      "         252       0.00      0.00      0.00      2616\n",
      "         253       0.00      0.00      0.00      7077\n",
      "         254       0.00      0.00      0.00      2102\n",
      "         255       0.00      0.00      0.00      1732\n",
      "         256       0.00      0.00      0.00       885\n",
      "         257       0.00      0.00      0.00      1097\n",
      "         258       0.00      0.00      0.00      3525\n",
      "         259       0.00      0.00      0.00       477\n",
      "         260       0.00      0.00      0.00       915\n",
      "         261       0.00      0.00      0.00       310\n",
      "         262       0.00      0.00      0.00       160\n",
      "         263       0.00      0.00      0.00      5545\n",
      "         264       0.00      0.00      0.00      1100\n",
      "         265       0.00      0.00      0.00      1413\n",
      "         266       0.00      0.00      0.00      7596\n",
      "         267       0.00      0.00      0.00        79\n",
      "         268       0.00      0.00      0.00       780\n",
      "         269       0.00      0.00      0.00      1206\n",
      "         270       0.00      0.00      0.00       499\n",
      "         271       0.00      0.00      0.00      1939\n",
      "         272       0.00      0.00      0.00       706\n",
      "         273       0.00      0.00      0.00      1812\n",
      "         274       0.00      0.00      0.00      2137\n",
      "         275       0.00      0.00      0.00        81\n",
      "         276       0.00      1.00      0.00       261\n",
      "         277       0.00      0.00      0.00       892\n",
      "         278       0.00      0.00      0.00       784\n",
      "         279       0.00      0.00      0.00      5920\n",
      "         280       0.00      0.00      0.00       241\n",
      "         281       0.00      0.00      0.00      7297\n",
      "         282       0.00      0.00      0.00         5\n",
      "         283       0.00      0.00      0.00       963\n",
      "         284       0.00      0.00      0.00       150\n",
      "         285       0.00      0.00      0.00        49\n",
      "         286       0.00      0.00      0.00        31\n",
      "         287       0.00      0.00      0.00        83\n",
      "         288       0.00      0.00      0.00       206\n",
      "         289       0.00      0.00      0.00        75\n",
      "         290       0.00      0.00      0.00       161\n",
      "         291       0.00      0.00      0.00      4062\n",
      "         292       0.00      0.00      0.00      1036\n",
      "         293       0.00      0.00      0.00       728\n",
      "         294       0.00      0.00      0.00         2\n",
      "         295       0.00      0.00      0.00       745\n",
      "         296       0.00      0.00      0.00       141\n",
      "         297       0.00      0.00      0.00       201\n",
      "         298       0.00      0.00      0.00       902\n",
      "         299       0.00      0.00      0.00       255\n",
      "         300       0.00      0.00      0.00       329\n",
      "         301       0.00      0.00      0.00       178\n",
      "         302       0.00      0.00      0.00      1557\n",
      "         303       0.00      0.00      0.00       137\n",
      "         304       0.00      0.00      0.00      1780\n",
      "         305       0.00      0.00      0.00      3881\n",
      "         306       0.00      0.00      0.00       301\n",
      "         307       0.00      0.00      0.00      2893\n",
      "         308       0.00      0.00      0.00       539\n",
      "         309       0.00      0.00      0.00     12264\n",
      "         310       0.00      0.00      0.00      1843\n",
      "         311       0.00      0.00      0.00      8258\n",
      "         312       0.00      0.00      0.00      7349\n",
      "         313       0.00      0.00      0.00       358\n",
      "         314       0.00      0.00      0.00      8368\n",
      "         315       0.00      0.00      0.00       502\n",
      "         316       0.00      0.00      0.00      7581\n",
      "         317       0.00      0.00      0.00      4733\n",
      "         318       0.00      0.00      0.00      8314\n",
      "         319       0.00      0.00      0.00      8239\n",
      "         320       0.00      0.00      0.00      1840\n",
      "         321       0.00      0.00      0.00      4989\n",
      "         322       0.00      0.00      0.00      5961\n",
      "         323       0.00      0.00      0.00      3197\n",
      "         324       0.00      0.00      0.00      3703\n",
      "         325       0.00      0.00      0.00      8404\n",
      "         326       0.00      0.00      0.00      2492\n",
      "         327       0.00      0.00      0.00      5207\n",
      "         328       0.00      0.00      0.00      3364\n",
      "         329       0.00      0.00      0.00       349\n",
      "         330       0.00      0.00      0.00      4557\n",
      "         331       0.00      0.00      0.00       879\n",
      "         332       0.00      0.00      0.00       677\n",
      "         333       0.00      0.00      0.00       458\n",
      "         334       0.00      0.00      0.00      1193\n",
      "         335       0.00      0.00      0.00       161\n",
      "         336       0.00      0.00      0.00      3096\n",
      "         337       0.00      0.00      0.00      1960\n",
      "         338       0.00      0.00      0.00      5798\n",
      "         339       0.00      0.00      0.00      2015\n",
      "         340       0.00      0.00      0.00       528\n",
      "         341       0.00      0.00      0.00      7325\n",
      "         342       0.00      0.00      0.00       111\n",
      "         343       0.00      0.00      0.00       380\n",
      "         344       0.00      1.00      0.01       831\n",
      "         345       0.00      0.00      0.00       718\n",
      "         346       0.00      0.00      0.00        80\n",
      "         347       0.00      0.00      0.00      2500\n",
      "         348       0.00      0.00      0.00        51\n",
      "         349       0.00      0.00      0.00       844\n",
      "         350       0.00      0.00      0.00        41\n",
      "         351       0.00      0.00      0.00      5190\n",
      "         352       0.00      0.00      0.00      1220\n",
      "         353       0.00      0.00      0.00        77\n",
      "         354       0.00      0.00      0.00       368\n",
      "         355       0.00      0.00      0.00       241\n",
      "         356       0.00      0.00      0.00        35\n",
      "         357       0.00      0.00      0.00       307\n",
      "         358       0.00      0.00      0.00       181\n",
      "         359       0.00      0.00      0.00      2469\n",
      "         360       0.00      0.00      0.00      1639\n",
      "         361       0.00      0.00      0.00      8745\n",
      "         362       0.00      0.00      0.00       221\n",
      "         363       0.00      1.00      0.01       521\n",
      "         364       0.00      0.00      0.00     10877\n",
      "         365       0.00      0.00      0.00      7341\n",
      "         366       0.00      0.00      0.00     20354\n",
      "         367       0.00      0.00      0.00      6873\n",
      "         368       0.00      0.00      0.00       365\n",
      "         369       0.00      0.00      0.00       790\n",
      "         370       0.00      1.00      0.00         9\n",
      "         371       0.00      0.00      0.00      1883\n",
      "         372       0.00      0.00      0.00      1626\n",
      "         373       0.00      0.00      0.00       334\n",
      "         374       0.00      0.00      0.00       542\n",
      "         375       0.00      0.00      0.00      3276\n",
      "         376       0.00      0.00      0.00       145\n",
      "         377       0.00      0.00      0.00       193\n",
      "         378       0.00      0.00      0.00       149\n",
      "         379       0.00      0.00      0.00       237\n",
      "         380       0.00      0.00      0.00        52\n",
      "         381       0.00      0.00      0.00       167\n",
      "         382       0.00      0.00      0.00      9491\n",
      "         383       0.00      0.00      0.00       327\n",
      "         384       0.00      0.00      0.00      3500\n",
      "         385       0.00      0.00      0.00       304\n",
      "         386       0.00      0.00      0.00      1119\n",
      "         387       0.00      0.00      0.00      2439\n",
      "         388       0.00      0.00      0.00      3186\n",
      "         389       0.00      0.00      0.00       510\n",
      "         390       0.00      0.00      0.00      2678\n",
      "         391       0.00      0.00      0.00      8812\n",
      "         392       0.00      0.00      0.00       483\n",
      "         393       0.00      0.00      0.00        89\n",
      "         394       0.00      0.00      0.00        10\n",
      "         395       0.00      0.00      0.00       844\n",
      "         396       0.00      0.00      0.00       115\n",
      "         397       0.00      0.00      0.00       214\n",
      "         398       0.00      0.00      0.00        62\n",
      "         399       0.00      0.00      0.00      1287\n",
      "         400       0.00      0.00      0.00       630\n",
      "         401       0.00      0.00      0.00      2745\n",
      "         402       0.00      0.00      0.00       665\n",
      "         403       0.00      0.00      0.00      2771\n",
      "         404       0.00      0.00      0.00        78\n",
      "         405       0.00      0.00      0.00      3899\n",
      "         406       0.00      0.00      0.00      2417\n",
      "         407       0.00      0.00      0.00     12449\n",
      "         408       0.00      0.00      0.00       966\n",
      "         409       0.00      0.00      0.00      2604\n",
      "         410       0.00      0.00      0.00        42\n",
      "         411       0.00      0.00      0.00      4001\n",
      "         412       0.00      0.00      0.00      1946\n",
      "         413       0.00      0.00      0.00      7966\n",
      "         414       0.00      0.00      0.00      6991\n",
      "         415       0.00      0.00      0.00      6901\n",
      "         416       0.00      0.00      0.00      6411\n",
      "         417       0.00      0.00      0.00      2102\n",
      "         418       0.00      0.00      0.00      5656\n",
      "         419       0.00      0.00      0.00       211\n",
      "         420       0.00      0.00      0.00        16\n",
      "         421       0.00      0.00      0.00      1403\n",
      "         422       0.00      0.00      0.00       186\n",
      "         423       0.00      0.00      0.00      7888\n",
      "         424       0.00      0.00      0.00      7383\n",
      "         425       0.00      0.00      0.00      5425\n",
      "         426       0.00      0.00      0.00      1923\n",
      "         427       0.00      0.00      0.00       582\n",
      "         428       0.00      0.00      0.00      1545\n",
      "         429       0.00      0.00      0.00       702\n",
      "         430       0.00      0.00      0.00     25583\n",
      "         431       0.00      0.00      0.00      2384\n",
      "         432       0.00      0.00      0.00      6065\n",
      "         433       0.00      0.00      0.00      1265\n",
      "         434       0.00      0.00      0.00      3262\n",
      "         435       0.00      0.00      0.00      1224\n",
      "         436       0.00      0.00      0.00      4844\n",
      "         437       0.00      0.00      0.00      1480\n",
      "         438       0.00      0.00      0.00      5363\n",
      "         439       0.00      0.00      0.00      2662\n",
      "         440       0.00      0.00      0.00      3075\n",
      "         441       0.00      0.00      0.00      3656\n",
      "         442       0.00      0.00      0.00      4312\n",
      "         443       0.00      0.00      0.00      2481\n",
      "         444       0.00      0.00      0.00        15\n",
      "         445       0.00      0.00      0.00        85\n",
      "         446       0.00      0.00      0.00        66\n",
      "         447       0.00      0.00      0.00       240\n",
      "         448       0.00      0.00      0.00      1234\n",
      "         449       0.00      0.00      0.00      2039\n",
      "         450       0.00      0.00      0.00      1305\n",
      "         451       0.00      0.00      0.00      1903\n",
      "         452       0.00      0.00      0.00      1313\n",
      "         453       0.00      0.00      0.00      3671\n",
      "         454       0.00      0.00      0.00       170\n",
      "         455       0.00      0.00      0.00       812\n",
      "         456       0.00      0.00      0.00      2016\n",
      "         457       0.00      0.00      0.00       585\n",
      "         458       0.00      0.00      0.00     34776\n",
      "         459       0.00      0.00      0.00       568\n",
      "         460       0.00      0.00      0.00       222\n",
      "         461       0.00      0.00      0.00       426\n",
      "         462       0.00      0.00      0.00        46\n",
      "         463       0.00      0.00      0.00      1212\n",
      "         464       0.00      0.00      0.00      2312\n",
      "         465       0.00      0.00      0.00       152\n",
      "         466       0.00      0.00      0.00       476\n",
      "         467       0.00      0.00      0.00      3049\n",
      "         468       0.00      0.00      0.00       657\n",
      "         469       0.00      0.00      0.00       279\n",
      "         470       0.00      0.00      0.00       106\n",
      "         471       0.00      0.00      0.00       291\n",
      "         472       0.00      0.00      0.00      2116\n",
      "         473       0.00      0.00      0.00      1357\n",
      "         474       0.00      0.00      0.00      2762\n",
      "         475       0.00      0.00      0.00      1233\n",
      "         476       0.00      0.00      0.00       599\n",
      "         477       0.00      0.00      0.00       215\n",
      "         478       0.00      0.00      0.00       138\n",
      "         479       0.00      0.00      0.00      1022\n",
      "         480       0.00      0.00      0.00       251\n",
      "         481       0.00      0.00      0.00       155\n",
      "         482       0.00      0.00      0.00      3750\n",
      "         483       0.00      0.00      0.00      7755\n",
      "         484       0.00      0.00      0.00      2722\n",
      "         485       0.00      0.00      0.00      1406\n",
      "         486       0.00      0.00      0.00      3005\n",
      "         487       0.00      0.00      0.00       196\n",
      "         488       0.00      0.00      0.00       441\n",
      "         489       0.00      0.00      0.00       808\n",
      "         490       0.00      0.00      0.00       451\n",
      "         491       0.00      0.00      0.00      1836\n",
      "         492       0.00      0.00      0.00      5349\n",
      "         493       0.00      0.00      0.00      6399\n",
      "         494       0.00      0.00      0.00      2307\n",
      "         495       0.00      0.00      0.00       948\n",
      "         496       0.00      0.00      0.00      3938\n",
      "         497       0.00      0.00      0.00       124\n",
      "         498       0.00      0.00      0.00      2331\n",
      "         499       0.00      0.00      0.00      1059\n",
      "\n",
      "   micro avg       0.01      0.01      0.01   1706646\n",
      "   macro avg       0.00      0.01      0.00   1706646\n",
      "weighted avg       0.00      0.01      0.00   1706646\n",
      " samples avg       0.01      0.00      0.00   1706646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irfan_lkd13dt\\miniconda3\\envs\\aml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Hamming Loss\n",
    "hamming_loss = metrics.hamming_loss(y_true, y_pred)\n",
    "print(f'Hamming Loss: {hamming_loss:.4f}')\n",
    "hamming_accuracy = 1 - hamming_loss\n",
    "print(f'Hamming Accuracy: {hamming_accuracy:.4f}')\n",
    "\n",
    "# Jaccard Score\n",
    "jaccard_score_micro= metrics.jaccard_score(y_true, y_pred, average='micro')\n",
    "print(f'Jaccard Score_micro: {jaccard_score_micro:.4f}')\n",
    "jaccard_score_macro= metrics.jaccard_score(y_true, y_pred, average='macro')\n",
    "print(f'Jaccard Score-macro: {jaccard_score_macro:.4f}')\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print(classification_report(y_true,y_pred, target_names = species.sort()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irfan_lkd13dt\\amlcoursework\\Baseline_(BCE)_model.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tpr \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m roc_auc \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(predictions\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     fpr[i], tpr[i], _ \u001b[39m=\u001b[39m roc_curve(true_labels[:, i], predictions[:, i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irfan_lkd13dt/amlcoursework/Baseline_%28BCE%29_model.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     roc_auc[i] \u001b[39m=\u001b[39m auc(fpr[i], tpr[i])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(predictions.shape[1]):\n",
    "    fpr[i], tpr[i], _ = roc_curve(true_labels[:, i], predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(predictions.shape[1])]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(predictions.shape[1]):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= predictions.shape[1]\n",
    "\n",
    "roc_auc[\"macro\"] = auc(all_fpr, mean_tpr)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(all_fpr, mean_tpr, label=f'Macro-average ROC Curve (AUC = {roc_auc[\"macro\"]:.2f})', linestyle='--', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random', alpha=0.8)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Macro-average ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
